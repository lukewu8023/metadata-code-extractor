# Testing Strategy - Design Document\n\n## 1. Overview\nThis document outlines the testing strategy for the Metadata Code Extractor project. A comprehensive testing approach is essential to ensure the reliability, correctness, and robustness of the application, especially given its reliance on LLMs and complex data interactions.\n\n## 2. Goals of Testing\n- Verify the functional correctness of individual components (unit tests).\n- Ensure seamless interaction and data flow between components (integration tests).\n- Validate the end-to-end metadata extraction workflow (end-to-end tests).\n- Ensure the system handles various inputs and edge cases gracefully.\n- Catch regressions as new features are added or code is refactored.\n- Evaluate the quality and accuracy of LLM-driven extractions and reasoning (more challenging, requires specific approaches).\n\n## 3. Testing Levels and Types\n\n### 3.1. Unit Tests\n- **Focus:** Individual functions, methods, and classes in isolation.\n- **Tools:** `pytest` will be the primary testing framework.\n- **Scope:**\n    - Logic within `LLMOrchestratorAgent` (e.g., state transitions, decision points based on mock inputs).\n    - Parsers within `CodeMetadataScanner` and `DocumentScanner` (e.g., testing specific language constructs or document formats with sample inputs).\n    - Rule logic within `CompletenessEvaluator` (e.g., testing if a rule correctly identifies a gap given a mock graph state).\n    - Utility functions, Pydantic model validation.\n    - `LLMClient` and `DBInterface` adapters (mocking the actual external calls).\n- **Mocking:** `unittest.mock` (or `pytest-mock`) will be used extensively to isolate units from external dependencies (LLMs, DBs, file system).\n\n### 3.2. Integration Tests\n- **Focus:** Interactions between two or more components.\n- **Tools:** `pytest`.\n- **Scope:**\n    - Agent orchestrating a scanner: `LLMOrchestratorAgent` -> `CodeMetadataScanner` -> (mocked) `LLMIntegration` and (mocked/test instance) `DBInterface`.\n    - Scanners interacting with `LLMIntegration`: `CodeMetadataScanner` -> `LLMIntegration` (testing prompt formatting and response parsing with a mocked LLM provider that returns predefined responses).\n    - Scanners/Agent interacting with actual test instances of DBs (e.g., local Neo4j, local ChromaDB/FAISS) with pre-populated test data.\n    - `CompletenessEvaluator` querying a test Graph DB instance.\n- **Data:** Requires well-defined test datasets for code and documents, and expected metadata output for verification.\n\n### 3.3. End-to-End (E2E) Tests\n- **Focus:** Testing the entire application workflow from input (code/document repositories) to output (populated Graph DB and Vector DB, identified gaps).\n- **Tools:** `pytest` orchestrating application runs (potentially via its CLI if one is developed).\n- **Scope:**\n    - Run the full extraction process on small, well-defined sample code and document repositories.\n    - Verify the final state of the (test) Graph DB and Vector DB against expected metadata.\n    - Check the `MetadataGap`s identified.\n- **Challenges:** These tests can be slow and complex to set up and maintain. LLM responses introduce non-determinism, which needs careful handling (see section 4).\n\n### 3.4. LLM-Specific Testing (Evaluation)\n- This is less about traditional pass/fail and more about evaluating the quality of LLM outputs.\n- **Focus:** Accuracy of metadata extraction, relevance of semantic search results, coherence of agent reasoning.\n- **Approach:**\n    - **Golden Datasets:** Manually curated datasets of code/documents with ideal extracted metadata.\n    - **Metrics:** Precision, recall, F1-score for structured extraction tasks. Semantic similarity scores for embedding-based tasks.\ Human evaluation for qualitative aspects (e.g., quality of summaries, reasoning traces).\n    - **Prompt Engineering/Evaluation Frameworks:** Potentially use tools or scripts to systematically test different prompts against a benchmark dataset and compare results.\n    - **Snapshot Testing for LLM outputs:** For some LLM interactions where the input is fixed, store the expected LLM output (or key parts of it) as a snapshot and compare. This is brittle to LLM model updates but can catch unexpected regressions in prompt formatting or critical LLM behavior changes.\n\n## 4. Handling Non-Determinism (LLMs)\n- **Mocking LLM Responses:** For unit and many integration tests, mock the `LLMIntegration` component to return predefined, consistent responses. This makes tests deterministic.\n- **Seeding (where applicable):** Some LLM APIs might offer temperature settings close to 0 for more deterministic output, but this isn\'t always sufficient.\n- **Focus on Structure and Key Information:** For E2E tests involving real LLM calls, assertions might focus on the presence of key information or correct structural formatting rather than exact string matches of LLM-generated text (like descriptions).\n- **Retry and Averaging (for evaluations):** When evaluating LLM quality with metrics, running tests multiple times and averaging scores can help mitigate randomness.\n- **Separate \"Online\" Tests:** Tests that make real LLM calls should be marked specially (e.g., `pytest.mark.online`) and potentially run less frequently (e.g., nightly) due to cost and flakiness.\n\n## 5. Test Infrastructure and Environment\n- **Test Runner:** `pytest`.\n- **Directory Structure:** Tests will reside in a `tests/` directory, mirroring the application structure (e.g., `tests/scanners/test_code_scanner.py`).\n- **Test Data:** Sample code files, document files, and expected output data will be stored in `tests/data/`.\n- **CI/CD:** Automated execution of tests (unit and integration) on every commit/PR using GitHub Actions or a similar CI service.\n- **Test Database Instances:** For integration tests, use ephemeral or easily resetable instances of Neo4j and a vector DB (e.g., Dockerized instances, local file-based DBs like ChromaDB/FAISS).\n\n## 6. Test Coverage\n- Aim for high unit test coverage for critical logic.\n- Focus integration tests on key interaction points and data flows.\n- E2E tests will cover common use cases and critical paths.\n- Code coverage tools (e.g., `pytest-cov`) will be used to monitor coverage, but high coverage alone isn\'t a guarantee of quality.\n\n## 7. Key Areas for Test Case Development\n\n- **LLM Orchestrator Agent:**\n    - Correct phase transitions.\n    - Decision logic for choosing next actions based on mock gaps/scan results.\n    - Handling of empty or error responses from other components.\n- **Scanners (Code & Document):**\n    - Correct parsing of various language features/document formats.\n    - Accurate extraction of entities, fields, relationships based on sample inputs.\n    - Correct chunking and embedding generation (mocking the embedding model itself).\n    - Correct handling of targeted scan parameters.\n- **Completeness Evaluator:**\n    - Each rule correctly identifies or ignores gaps based on mock graph data.\n    - Correct creation and population of `MetadataGap` objects.\n- **LLM Integration:**\n    - Correct prompt formatting and parameter substitution.\n    - Correct parsing of mock LLM responses.\n    - Cache hits and misses.\n    - Adapter logic for different (mocked) LLM providers.\n- **Database Integration:**\n    - CRUD operations for all node and relationship types against a test DB instance.\n    - Correctness of semantic search results from a test Vector DB instance.\n    - Idempotency of `ensure_node/ensure_relationship`.\n\n## 8. Future Enhancements\n- Performance testing to identify bottlenecks.\n- Security testing, especially if the application handles sensitive code or is exposed via an API.\n- Fuzz testing for input validation.\n- More sophisticated LLM evaluation pipelines and dashboards. 